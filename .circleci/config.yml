version: 2.1
orbs:
  aws-eks: circleci/aws-eks@1.0.3
  aws-ecr: circleci/aws-ecr@6.15.3
  kubernetes: circleci/kubernetes@0.11.2
  slack: circleci/slack@4.1

commands:
  destroy-environment:
    steps:
      - run:
          name: Destroy environments
          when: on_fail
          command: |
            echo "DESTROY STACKS"
            echo "Destroyed ?????"
jobs:
#--------------------- Job 1 --------------------------
  linting:
    docker:
      - image: python:3.7.3-stretch
    working_directory: ~/project
    steps:
      - checkout
      - run:
          name: checking paths
          command: |
            echo 'path is:' ${pwd}
            echo 'the path is:'
            pwd
            ls -la
            ls -la ~/project/.circleci/dockerstuff
      # Download and cache dependencies
      - restore_cache:
          keys:
            - v1-dependencies-{{ checksum "~/project/.circleci/dockerstuff/requirements.txt" }}
            # fallback to using the latest cache if no exact match is found
            - v1-dependencies-

      - run:
          name: install dependencies
          command: |
            cd ~/project/.circleci/dockerstuff
            python3 -m venv venv
            ls -l venv
            ls -l venv/bin
            . venv/bin/activate
            apt-get -y install make
            make --version
            # Install hadolint
            wget -O /bin/hadolint https://github.com/hadolint/hadolint/releases/download/v1.16.3/hadolint-Linux-x86_64 &&\
              chmod +x /bin/hadolint
      - save_cache:
            paths:
              - ./venv
            key: v1-dependencies-{{ checksum "~/project/.circleci/dockerstuff/requirements.txt" }}

      # run lint!
      - run:
          name: Run lint
          command: |
            cd ~/project/.circleci/dockerstuff
            . venv/bin/activate
            make install
            make lint
      - slack/notify:
          event: fail
          template: basic_fail_1

#--------------------- Job 2 --------------------------
  eksctl-install:
    #executor: aws-eks/python3
    machine: true

    steps:
      - checkout
      - add_ssh_keys:
          fingerprints: ["a3:27:68:1c:b4:68:20:dd:2a:d7:5e:79:3d:06:77:46"]
      - run:
          name: install eksctl
          command: |
            curl --silent --location "https://github.com/weaveworks/eksctl/releases/latest/download/eksctl_$(uname -s)_amd64.tar.gz" | tar xz -C /tmp
            sudo mv /tmp/eksctl /usr/local/bin
            echo "eksctl version: " $(sudo eksctl version)
      - run:
          name: install kubectl
          command: |
            curl -Lo kubectl https://storage.googleapis.com/kubernetes-release/release/v1.19.0/bin/linux/amd64/kubectl && chmod +x kubectl && sudo mv kubectl /usr/local/bin/
            kubectl version --short --client
      - run:
          name: create cluster
          command: |
            pwd
            #ls -la ~/project/k8s
            cd ~/project/eks
            aws configure set aws_access_key_id $AWS_ACCESS_KEY_ID
            aws configure set aws_secret_access_key $AWS_SECRET_ACCESS_KEY
            aws configure set role_arn $ROLE_ARN --profile circleci
            eksctl create cluster --name mycapstone --version 1.16 --nodegroup-name standard-workers --node-type t2.micro --nodes 3 --nodes-min 1 --nodes-max 4 --node-ami auto --region us-east-1 --zones us-east-1a,us-east-1b,us-east-1c
      - run:
          name: delete cluster
          command: |
            #sudo eksctl delete cluster --name mycapstone --region us-east-1
            echo
            kubectl get services
      - slack/notify:
          event: fail
          template: basic_fail_1

#--------------------- Job 3 --------------------------
  check-cluster:
    executor: aws-eks/python3
    parameters:
      cluster-name:
        description: |
          Name of the EKS cluster
        type: string
    steps:
      - kubernetes/install
      - aws-eks/update-kubeconfig-with-authenticator:
          cluster-name: << parameters.cluster-name >>
      - run:
          name: Kluster info
          command: |
            kubectl get services
            #aws eks update-kubeconfig --name mycapstone --region us-east-1
            kubectl get nodes

#--------------------- Job 4 --------------------------
# build and upload image (see under workflow)

#--------------------- Job 5 --------------------------
  deploy-application:
    executor: aws-eks/python3
    parameters:
      cluster-name:
        description: |
          Name of the EKS cluster
        type: string
      docker-image-name:
        description: |
          Name of the docker image to be deployed
        type: string
      version-info:
        description: |
          App version information
        type: string
      aws-region:
        description: |
          AWS region
        type: string
        default: ""
      account-url:
        description: |
          Docker AWS ECR repository url
        type: string
      tag:
        description: |
          Docker image tag
        type: string
    steps:
      - checkout
      - run:
          name: create deployment  template
          command: |
            pwd
            ls -l
            BUILD_DATE=$(date '+%Y%m%d%H%M%S')
            cat k8s/deployment.tpl |\
            sed "s|DOCKER_IMAGE_NAME|<< parameters.docker-image-name >>|" |\
            sed "s|DOCKER_REPO_NAME|<< parameters.account-url >>|" |\
            sed "s|DOCKER_IMAGE_TAG|<< parameters.tag >>|" > k8s/deployment.yml
            cat k8s/deployment.yml

      - aws-eks/update-kubeconfig-with-authenticator:
          cluster-name: << parameters.cluster-name >>
          install-kubectl: true
          aws-region: << parameters.aws-region >>
      - kubernetes/create-or-update-resource:
          resource-file-path: "k8s/deployment.yml"
          get-rollout-status: true
          resource-name: deployment/capstone
          action-type: apply
          show-kubectl-command: true
      - kubernetes/create-or-update-resource:
          action-type: apply
          resource-file-path: "k8s/service.yml"
          show-kubectl-command: true
      - slack/notify:
          event: fail
          template: basic_fail_1

#--------------------- Job 6 --------------------------
  test-application:
    executor: aws-eks/python3
    parameters:
      cluster-name:
        description: |
          Name of the EKS cluster
        type: string
      aws-region:
        description: |
          AWS region
        type: string
        default: ""
      expected-version-info:
        description: |
          Expected app version (this is used for testing that the
          correct version has been deployed)
        type: string
    steps:
      - aws-eks/update-kubeconfig-with-authenticator:
          cluster-name: << parameters.cluster-name >>
          install-kubectl: true
          aws-region: << parameters.aws-region >>
      - run:
          name: Wait for service to be ready
          command: |
            kubectl get pods
            kubectl get services
            sleep 30
            EXTERNAL_IP=$(kubectl get service demoapp | awk '{print $4}' | tail -n1)
            #EXTERNAL_PORT=$(kubectl get services --namespace=capstone | tail -n1 | awk '{ print $5 }' | cut -d : -f1)
            #sleep 180
            #curl -s "http://$EXTERNAL_IP:$EXTERNAL_PORT" | grep "hello"
            #curl -s "http://$EXTERNAL_IP | grep "hello"
      - slack/notify:
          event: fail
          template: basic_fail_1

#--------------------- workflow ------------------------
workflows:
  main:
    jobs:
      #- eksctl-install
      - check-cluster:
          cluster-name: mycapstone
          #requires:
          #  - eksctl-install
      - aws-ecr/build-and-push-image:
          account-url: AWS_ECR_URL
          region: AWS_DEFAULT_REGION
          repo: mycapstone
          dockerfile: Dockerfile
          path: .circleci/dockerstuff
          aws-access-key-id: AWS_ACCESS_KEY_ID
          aws-secret-access-key: AWS_SECRET_ACCESS_KEY
          create-repo: true
          tag: ${CIRCLE_SHA1}
          requires:
             - check-cluster
#      - aws-eks/create-cluster:
#          cluster-name: mycapstone
#          aws-region: $AWS_DEFAULT_REGION
#          requires:
#            - aws-ecr/build-and-push-image
      - deploy-application:
          cluster-name: mycapstone
          aws-region: $AWS_DEFAULT_REGION
          account-url: ${AWS_ECR_URL}
          docker-image-name: "${AWS_ECR_URL}/capstone:${CIRCLE_SHA1}"
          version-info: "${CIRCLE_SHA1}"
          tag: ${CIRCLE_SHA1}
          requires:
            - aws-ecr/build-and-push-image
      #- test-application:
      #    name: test-application
      #    cluster-name: mycapstone
      #    aws-region: $AWS_DEFAULT_REGION
      #    expected-version-info: "${CIRCLE_SHA1}"
      #    requires:
      #     - deploy-application
